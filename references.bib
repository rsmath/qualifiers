@book{fasshauer2015kernel,
  title={Kernel-based approximation methods using Matlab},
  author={Fasshauer, Gregory E and McCourt, Michael J},
  volume={19},
  year={2015},
  publisher={World Scientific Publishing Company}
}

@book{wendland2004scattered,
  title={Scattered data approximation},
  author={Wendland, Holger},
  volume={17},
  year={2004},
  publisher={Cambridge university press}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{zhang2024trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={49},
  pages={1--55},
  year={2024}
}

@article{biau2012analysis,
  title={Analysis of a random forests model},
  author={Biau, G{\'e}rard},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={1063--1095},
  year={2012},
  publisher={JMLR. org}
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{liu2024neural,
  title={Neural scaling laws of deep relu and deep operator network: A theoretical study},
  author={Liu, Hao and Zhang, Zecheng and Liao, Wenjing and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2410.00357},
  year={2024}
}

@article{lu2019deeponet,
  title={Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1910.03193},
  year={2019}
}

@article{de2022cost,
  title={The cost-accuracy trade-off in operator learning with neural networks},
  author={de Hoop, Maarten V and Huang, Daniel Zhengyu and Qian, Elizabeth and Stuart, Andrew M},
  journal={arXiv preprint arXiv:2203.13181},
  year={2022}
}

@article{lanthaler2022error,
  title={Error estimates for deeponets: A deep learning framework in infinite dimensions},
  author={Lanthaler, Samuel and Mishra, Siddhartha and Karniadakis, George E},
  journal={Transactions of Mathematics and Its Applications},
  volume={6},
  number={1},
  pages={tnac001},
  year={2022},
  publisher={Oxford University Press}
}

@article{ingebrand2025basis,
  title={Basis-to-basis operator learning using function encoders},
  author={Ingebrand, Tyler and Thorpe, Adam J and Goswami, Somdatta and Kumar, Krishna and Topcu, Ufuk},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={435},
  pages={117646},
  year={2025},
  publisher={Elsevier}
}
