{\bf Answer:}

% universal approximation theorems and what they are
Universal approximation theorems (UAT) are a critical part of machine learning research. They provide theoretical guarantees on the approximation capabilities of neural networks. This often results in judiciously picking the free parameters (hyperparameters such as learning rate, network width and height, activation function, etc.) in accordance with an architecture's UAT. Early work showed single layer networks with an infinite number of neurons can approximate arbitrary functions~\citep{irie1988capabilities}, or a single layer network with cosine activation can give Fourier series approximation for square-integrable target functions~\citep{gallant1988there}. But for fitting arbitrary continuous functions, some of the first works came from~\cite{cybenko1989approximation, hornik1989multilayer} which show that wide shallow neural networks using continuous or monotonic sigmoid functions can arbitrarily approximate arbitrary continuous functions. To illustrate a fundamental UAT, we briefly go over Theorem 1 from~\citep{cybenko1989approximation}.
\paragraph{Theorem 1}
\begin{theorem}
Let $I_n$ denote the $[0, 1]^n$ unit hypercube and $C(I_n)$ the space of continuous functions in that hypercube. Loosely speaking, we say a scalar valued function $\sigma$ is discriminatory if it can detect the sign of its input (acting as an activation function). We are interested in finite sums of the form
\begin{align}
M(x) = \sum\limits_{j=1}^N \alpha_j \sigma(y_j^{\top} x + \theta_j),
\end{align}
where $x,y \in \R^n$ and $\theta, \alpha \in \R$. 
\end{theorem}
Then, given $f \in C(I_n)$ and $\epsilon > 0$, there is a sum $M(x)$ such that $|M(x) - f(x)| < \epsilon, \forall x \in I_n$.
\begin{proof}
Let $S \subset C(I_n)$ be the space of functions to which $M$ belongs. This means $S$ is a linear subspace of $C(I_n)$. Assuming this were not true, according to the Hahn-Banach theorem and Riesz Representation theorem, there is a bounded linear functional on $C(I_n)$ that is zero on $S$. We write it as
\begin{align}
\int\limits_{I_n} \sigma(y^{\top} x + \theta) d \mu(x) = 0, ; \forall \y, \theta,
\end{align}
where $\mu$ is some Borel measure (required for $\sigma$ to be discriminatory). Sine we assume $\sigma$ is discriminatory, therefore $\mu = 0$, this means that our assumption is wrong, and the linear functional is zero everywhere (not just on $S$). This shows that sums of the form $M(x)$ are dense in $C(I_n)$ provided $\sigma$ is discriminatory and continuous.
\end{proof}
\citep{cybenko1989approximation} showed similar theorems and proofs for sigmoidal $\sigma$ (a function that is 1 on the positive real line, zero on the negative real line). However, they conclude with the catuious statement that their proofs only show the \emph{existence} of such powerful networks but that finding the correct architecture for them is a question for feasibility.~\citep{shen2022optimal} show the approximation rates for networks using the ReLU activation function in terms of the width and depth.~\citep{leshno1993multilayer} showed that as long as the chosen activation function is not a polynomial, multilayer perceptrons can approximate any function to an arbitrary accuracy. While these results are from the perspective of increasing the width, the alternate perspective has also been explored. For example,~\citep{lu2017expressive, park2020minimum} derive the minimum width necessary for arbitrarily deep neural networks to be universal approximators; found to be $\text{max}(n + 1, m)$ where the functions being approximated are of the form $f: \R^n \rightarrow \R^m$. The tradeoff between increasing the width and depth to fit different classes of functions is covered well by~\citep{telgarsky2016benefits}. In general, wide and shallow networks need exponentially more neurons in each layer to approximate a function with the same accuracy as a deep network~\cite[Theorem 1.15]{holstermann2023expressive}.

UATs have also been thorougly investigated in the context of neural operators.~\citep{chen1995universal, chen1995approximation} came up with ``operator networks'' and showed that single layer neural networks can approximate to arbitrary accuracy any \emph{operators}. Interestingly,~\citep{chen1995approximation} used radial basis function (RBF) neural networks for their analysis. Here, we explore UATs for two popular neural operator architectures, deep operator networks (DeepONet) and Fourier neural operator (FNO).~\cite{lu2019deeponet}, inspired from this work, came up with the DeepONet architecture (an inner product of two separate neural networks). The following UAT for DeepONet~\citep{lu2019deeponet} is inspired by results in~\citep{chen1995universal},
\paragraph{UAT for DeepONet}
\begin{theorem}
Given a nonlinear continuous operator $G$, an activation function $\sigma$ that belongs to the class of Tauber-Wiener functions (scalar valued continuous/discontinuous functions whose linear combinations are dense in the range of continuous functions $C([a, b])$), and $A$ and $U$ are the input and output function spaces respectively, then for any $\epsilon > 0$, there exists a positive integer $p$ such that
\begin{align}
\left| G(a)(y) - \sum\limits_{i=1}^{p} \boldsymbol{\beta}_i(a(X), \boldsymbol{\theta}_{\beta}) \sigma(\boldsymbol{\tau}_i(y, \boldsymbol{\theta}_{\tau})) \right| < \epsilon,
\end{align}
where $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ are the branch and trunk networks respectively parametrized by neural network parameters $\boldsymbol{\theta}_{\beta}$ and $\boldsymbol{\theta}_{\tau}$ respectively, $X$ is a set of points in $D_a$, and $y \in D_u$.
\end{theorem}
However, the original DeepONet UAT does not cover many areas of practical interest, where the input space may not be compact and assumptions on continuity on $G$ may fail. It is also general enough that judiciously choosing $p$, cardinality of the set $X$ (number of location to sample functions in $A$ at), and the architecture of the branch and trunk networks is difficult. Recognizing this,~\citep{lanthaler2022error} comprehensively extended UATs for DeepONets and performed rigorous error analysis. Relaxing the norm to compute the error between $G$ and the DeepONet, they are able to remove the compactness requirement that the UAT in~\citep{lu2019deeponet} and~\citep{chen1995universal} had and are able to apply the theorem to the approximation of nonlinear operators of the kind that appear in~\citep{lu2019deeponet}. This was a key step in showing explicitly how DeepONets can break the curse of dimensionality.

\citep{kovachki2021universal} presented a UAT for the fourier neural operator (FNO) class of neural operators. Their UAT is as follows
\paragraph{UAT for FNO}
\begin{theorem}
FNOs can approximate an arbitrary operator $G$ to any desired accuracy; $\text{sup} \|G(a) - F(a)\|\limits_{H^d} < \epsilon, \forall a \in K$, where $F$ is the FNO, $G$ maps from functions in $H^s$ to functions in $H^d$, and $K \subset H^s$.
\begin{proof}
Let $G_N: H^s \rightarrow L^2, \; G_N(a) = P_N G(P_N a),$ where $P_N$ is the orthogonal Fourier projection operator. From this we have $\|G(a) - G_N(a)\|\limits_{L^2} \le \epsilon, \; \forall a \in K$. One needs to then show that $F$ can approximate $G_N$ with error $\epsilon$. Simplifying the next step, using Fourier dual operators we get the identity $G_N(a) = \F_N^{-1} \circ \hat{G}_N \circ \F_N (P_N a)$ where $\F$ and $\F^{-1}$ are the discrete Fourier and inverse Fourier transforms respectively. The key is to then show that FNOs can approximate each of the three operators, $\F_N^{-1}, \hat{G}_N, \F_N P_N$.
\end{proof}
\end{theorem}
{\bf Insights:} A common trend that emerges in UATs for both function and operator approximation by neural networks is to prove that the the architecture being belongs to forms of functions that are dense in $L(\Omega)$ where $L$ is some function space to which the output space $U$ belongs and $\Omega \subset \R^u$. While the UAT results for function approximation directly help in choosing the architecture hyperparameters for neural networks (such as the width, height, and activation), it is not always trivial to do so for neural operators. For example, the UAT for FNOs do not include the lifting and projection layers (often parametrized with MLPs) of the FNO architecture explicitly and therefore provide no insight regarding picking their hyperparameters. Similarly, in the DeepONet UAT, while the theorem states that the output functions can be approximated with arbitrary accuracy, there are no bounds that describe the optimal hyperparameters for the branch and trunk network \emph{simulataneously} (like the ones in~\cite{lu2017expressive, park2020minimum} for function approximation).

Even though it is difficult to pick the precise architecture hyperparameters \emph{a priori} for DeepONets and FNOs, slightly more relaxed analysis is still possible.~\citep{herrmann2024neural} prove that DeepONets with ReLU activation can approximate holomorphic operators with convergence rate $n^{1-s}$ (in the supremum norm) where $n$ is the number of trainable parameters and $s \sim 1/p$ where $p$ is the number of basis functions used in the DeepONet. Such analysis is more intuitive to do on the DeepONet than the FNO because DeepONets belong to a class of encoder-decoder style neural operator architectures~\citep{kovachki2024operator}, where encoding and decoding involve $A \approx \R^a$ and $\R^u \approx U$ respectively. For Lipschitz operators,~\citep{kovachki2024operator} also showed that if DeepONets and FNOs use standard MLPs, the number of parameters $n$ grows as $n \gtrsim \exp(c \epsilon^{-a/s})$, where $s$ is smoothness parameter.
% UAT for regular neural network architectures (finite dimensional function approximators is what they are), and how it helps neural operators (the popular ones)
% be specific how UAT **results** can be used to construct neural operator architectures.
% how can UAT results for functions be used to improve UAT results for neural operators, and go from approximation of functions to those of operators

Deriving universal approximation theorems for operators is an important yet difficult task. Most current neural operator architectures do not lend themselves to UATs with which their optimal hyperparameters can be found to achieve a desired error $\epsilon$. UATs for functions are useful in deriving only very general UATs for operators. In most encoder-decoder style architectures, the difficulty arises from the use of usually separate neural networks for encoding input functions and a basis for output functions. A potential novel approach that may mitigate this is to use a single network style architecture (those similar to the ones in~\citep{chen1995universal}). The challenge is to make something as expressive and accurate as the current state-of-the-art neural operators. The key to solving this may be an architecture that lies at the intersection of kernel based methods~\cite{batlle2024kernel} and neural networks, a personal research direction for the near future. Specifically for PDE applications, in this architecture it would be possible to imbibe kernels with desirable conservation properties analytically (something neural operators are known to struggle with~\citep{khorrami2024physics}), yet still benefit from the expressivity of neural networks.
%
\pagebreak
