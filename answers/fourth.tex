{\bf Answer:}
% \part Please introduce and summarize existing operator learning methods. Please give some categories, and summarize pros and cons for each category. \\
% {\bf  Recommendation: no less than 2 pages.}

{\bf Part a:} Scientific machine learning methods can be categorized into three areas~\citep{boulle2024mathematical}; (i) PDE solvers~\citep{raissi2019physics}, (ii) PDE discovery~\citep{brunton2016discovering}, and operator learning. Operator learning methods \emph{approximate} a ``function-to-function'' map between two separable infinite-dimensional function spaces. In the most general setting, consider two such Banach function spaces, $\U(\Omega_u; \|\cdot\|_{\R^{d_u}})$ and $\V(\Omega_v, \|\cdot\|_{\R^{d_v}})$ where $\Omega_u \subset \R^{d_u}$ and $\Omega_v \subset \R^{d_v}$. Then, given a finite number of function pairs $\{(u_1, v_1), (u_2, v_2), \dots, (u_N, v_N)\}$ where, $u_i \in \U$ (input functions) and $v_i \in \V$ (output functions), one aims to approximate the true operator $\G: \U \rightarrow \V$ with some approximation $\tilde{\G}$ such that $\|\tilde{\G} - \G \|$ is minimized.

In recent years, with the advent of machine learning being used for SciML applications, many neural network architectures have emerged for operator learning. We enumerate and explain below the popular methods, neural network and otherwise.
\begin{itemize}
\item {\bf DeepONet:} First introduced in~\citep{lu2019deeponet}, the DeepONet architecture consists of two networks, a branch network $\boldsymbol{\beta}: \R^{N_x} \rightarrow \R^p$ that takes an input function $u$ sampled at $N_x$ locations, and a trunk network $\boldsymbol{\tau}: \R^{d_v} \rightarrow \R^p$ that takes as input a $d_v$ dimensional location to evaluate the output function at. The DeepONet output then is the $p$-dimensional inner product between the branch output (can be thought of as coefficients) multipling the trunk output (a set of spatial basis functions for the output function space) $\tilde{\G}(u)(y) = \langle \boldsymbol{\tau}(y), \boldsymbol{\beta}(u) \rangle$, where $y$ is a location where the output function is evaluated and $u$ is sampled at some points $\{x_i\}\limits_{j=1}^{N_x} \in \Omega_u$. The two networks are trained by minimizing the loss function $\|\tilde{\G}(u_i)(y) - v_i(y) \|_2^2$ to some tolerance for all input-output function pairs (the output functions are sampled at $N_y$ points $\{y_k\}\limits_{k=1}^{N_y} \in \Omega_v$). In fact, the same loss function is used by all neural operator architectures albeit with different parametrizations of $\tilde{\G}$.

The DeepONet is a simple but effective architecture. It's architecture presents its output as a linear expansion of basis functions which allows problem-dependent basis functions to be picked~(POD-DeepONet in~\citep{lu2022comprehensive}, ensemble and partition-of-unity (PoU) DeepONets in~\citep{sharma2024ensemble}). However, while DeepONets are architecturally elegant and modifiable, they are generally outperformed by other neural operators.

\item {\bf FNO:}~\citep{li2020fourier} introduced one of the most popular neural operator architectures. The FNO parametrizes the integral kernel in the Fourier space. Their architecture consists of a lifting operator for the input functions to multiple ``channels'', followed by multiple Fourier layers with kernel integral operators that parametrize the kernel with neural networks directly in the Fourier space by using the fast Fourier transform (FFT), and finally a projection layer that undoes the lifting operation. Let $L$ and $P$ be the lift and projection operators respectively and $f_i$ denote the $i^{th}$ of $T$ intermediate Fourier layer. Then the FNO can be written as $\G(u)(y) = P(\; \sigma(\; f_T(\; \sigma(\; \dots f_2(\; \sigma(\; f_1(\; L(\; u(y))))) \dots ))))$, where $\sigma$ is a chosen nonlinear activation function. A given Fourier layer, say the $(t+1)^{th}$ one, can be written as
\begin{align}
f_{t+1}(y) &= \sigma\left(\mathcal{F}^{-1}\left( \mathcal{F}(f_t)(x) \right)(y) +  W f_t(y) \right),
\end{align}
where $\mathcal{F}$ and $\mathcal{F}^{-1}$ are the kernel integral operators (using translation invariant kernels) defined in Fourier space using FFT and IFFT respectively, and $W$ is a pointwise convolution operator. This architecture requires the functions to be sampled on points on equispaced grids, which has tremendous benefits as well as limitations. FNOs are naturally resolution-invariant and can do zero-shot super resolution (evaluate on a different resolution at inference time than the one it is trained on) accurately that other architectures such as CNNs cannot (\citep[Section 4]{li2020fourier}). Various FNO architectures have emerged over the years that mitigate two primary issues in the original method, (i) the requirement for the functions to be on regular grids (as need by the FFT algorithm), and (ii) output functions being evaluated on a different set of points than the input functions.~\citep{lu2022comprehensive} came up with the gFNO and dFNO architectures that mitigate both these issues.~\citep{li2023fourier} came up with the geometry-aware FNO architecture that can work with arbitrary domain geometries.~\citep{cao2024laplace} came up with the Laplace Neural Operator (LNO) which uses the pole-residue relationship between the input and output function spaces and outperforms FNO. Finally,~\citep{guo2024mgfno} introduced the multi-grid FNO architecture that uses a three-level hierarchy; three different networks, for coarse scale, intermediate scale, fine scale are trained simultaneously to achieve high-resolution accuracy.

\item {\bf Deep green networks:} This class of neural operators learn the kernel of the kernel integral operator directly in the physical space~\citep{gin2021deepgreen, boulle2022data}. It parametrizes $\tilde{\G}$ as
\begin{align}
\tilde{\G}(u)(y) = \int\limits_{\Omega} G(x, y) u(x) dx, \; x \in \Omega,
\end{align}
where $\Omega$ is the domain of the PDE whose solution operator is being learned and $G: \Omega \times \Omega$ is the Green's kernel learned from the data parametrized as a neural network~\citep{boulle2022data}. A key advantage here is that the Green's kernel can be visualized. To discretize the integral operator a valid quadrature rule needs to be found which scales quadratically with the number of points where the function is evaluated~\citep{boulle2024mathematical}.

\item {\bf Graph Neural Operator:}~\citep{li2020neural} came up with the graph neural operator (GNO) architecture which is similar to the deep green network (DGN) in that it aims to learn the Green's kernel. However, instead of learning the kernel globally like in DGN, GNO performs the integral operation locally on a ball of radius $r$, $B(x, r)$ around each point $x$. This leads to the choice of discretizing $\Omega$ with a graph whose nodes are the spatial points. GNO therefore instead approximates the local kernel $G_r$.~\citep{li2020neural} go over errors bounds for $\|G - G_r\|_{L^2(\Omega \times \Omega)}$ and show that the limited kernel $G_r$ well approximates $G$ especially in higher dimensions.~\citep{li2020multipole} build up on this idea with the {\bf Multipole graph neural operator} (MGNO) that learns both short- and long-range interactions in $G$ by decomposing the kernel into a sum of low rank kernels, $G = K_1 + K_2 + \dots + K_L$. This approach has the added advantage of evaluating the integral operation in linear complexity. Interestingly, MGNO is similar to DeepONets in that they both are low rank neural operators (because of the limitation of $p$ basis functions in DeepONets), but the MGNO architecture is more flexible since the kernel it is approximation, $G$, itself is not low-rank~\citep{boulle2024mathematical}!

\item {\bf Kernels:} Finally,~\citep{batlle2024kernel} show that kernel methods are indeed competitive for operator learning! Using the theory of reproducible Kernel Hilbert spaces (RKHS) and Gaussian Processes (GP), the authors come up with an operator learning framework well supported by convergence proofs and error bounds. In many examples this method outperforms popular neural operator architectures. Given a chosen kernel $K$ (for example the rational quadratic, Gaussian, or the class of Mat\'ern kernels), the method approximates output functions as
\begin{align}
\G(u) = K(u, U) K(U, U)^{-1} V,
\end{align}
where $U$ and $V$ are all the input and output functions ``stacked'' together as block vectors respectively. Since, $K(U, U)^{-1} V$ can be computed once and stored, the inference time of this method is significantly smaller compared to neural operators! The method also has mesh invariance (see Section 2.4). Another huge advantage of GPs in this method is that uncertainty quantification is possible.
\end{itemize}

% \part What do you think about the future of operator learning? You can explain whatever thoughts you have, positive, negative, and future development direction, etc. 
% {\bf Recommendation: no less than 1 page.} 
{\bf Part b:} The field of operator learning is growing fast! The methods outlined above are already being used in real world scientific applications. To list some, weather and climate modeling~\citep{bora2023learning, pathak2022fourcastnet, jiang2023efficient, yang2024fourier}, earthquake modeling~\citep{haghighat2024deeponet}, carbon sequestration~\citep{lee2024efficient}, ocean modeling~\citep{choi2024applications}, hydrology~\citep{sun2024bridging}, and material science~\citep{gupta2022towards, oommen2024rethinking}.

There are several directions of growth that the field is seeing. The first is a fundamental exploration of new neural operator architectures apart from the common ones listed above.~\citep{kurz2024radial} uses shallow radial basis function neural networks to come up with the first neural operator to learn entirely in both time and frequency domains, and achieve small error in both in-distribution and out-of-distribution tests.~\citep{ingebrand2025basis} came up with a novel methodology to learn the underlying basis functions for both the input and output function spaces to be able to evaluate them at arbitrary locations at inference time.~\citep{bhattacharya2021model} introduced the PCA-Net architecture that reduces the dimensionality of input and output functions to a lower dimensional latent space where it learns the operator map. While this is not an exhaustive list, the salient trend to highlight here is the emergence of innovative architectures that borrow inspiration from a variety of traditional machine learning methods to bring to operator learning.

To further that note, \citep{hao2023gnot} and \cite{liu2025geometry} are examples of the operator learning community moving to towards amalgamation with transformer style architectures.~\citep{hao2023gnot} designed a framework that allows for multiple input functions and irregular meshes. They also introduce a gating mechanism that can be viewed to help with multi-scale problems.~\citep{liu2025geometry} can make predictions on arbitrary geometries with surface point clouds that are unordered and have non-uniform point density on 2D and 3D problems, a remarkable feat compared to traditional neural operators. Other novel transformer based operator learning architectures include~\citep{cao2021choose, liu2024mitigating, li2022transformer}. This is a very promising direction to scale operator learning methods on scientific applications to extremely large datasets using the expressivity of transformers and the efficient parallelized computational methods they've been developed with since their advent.

Finally, a crucial step forward necessary for operator learning methods for scientific computing applications is trustworthiness. Physical constraints such as conservation laws are hard to accurately build in as hard constraints into neural network architectures. Physical-informed hard constraints result in increased computational cost which requires specialized methods to make them feasible~\citep{chalapathi2024scaling}. A specific property of interest is the incompressibility constraint of velocity fields in fluid dynamics, commonly written as the divergence-free constraint. While there is attempt on building this constraint exactly into an FNO~\citep{khorrami2024physics}, problems still remain. My current work revolves around a novel kernel based operator learning method that can analytically encode not just the incompressibility constraint but other desirable properties while maintaining state-of-the-art accuracy on fluid dynamics problems. This leads into the most promising future direction for operator learning; trustworthy and expressive neural operator architectures for scientific computing applications.
%
\pagebreak
