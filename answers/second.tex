{\bf Answer:}

Universal approximation theorems (UAT) are a critical part of machine learning research. They provide theoretical guarantees on the approximation capabilities of neural networks. This often helps with judiciously picking the free parameters (hyperparameters such as learning rate, network width and height, activation function, etc.) for a neural network architecture. Early work showed single layer networks with an infinite number of neurons can approximate arbitrary functions~\citep{irie1988capabilities} and single layer networks using the cosine activation function can give Fourier series approximation for square-integrable target functions~\citep{gallant1988there}. But for fitting arbitrary continuous functions, some of the first work came from~\citep{cybenko1989approximation} and~\citep{hornik1989multilayer} which showed that wide shallow networks using continuous or monotonic sigmoid activation functions can approximate arbitrary continuous functions with arbitrary accuracy. To illustrate a fundamental UAT, we briefly go over Theorem 1 from~\citep{cybenko1989approximation}.
\paragraph{Theorem 1}
\begin{theorem}
Let $I_n$ denote the $[0, 1]^n$ unit hypercube and $C(I_n)$ the space of continuous functions in that hypercube. Loosely speaking, we say that a scalar valued function $\sigma$ is discriminatory if it can detect the sign of its input (acting as an activation function). We are interested in finite sums of the form
\begin{align}
M(x) = \sum\limits_{j=1}^N \alpha_j \sigma(y_j^{\top} x + \theta_j),
\end{align}
where $x,y \in \R^n$ and $\theta, \alpha \in \R$. 
\end{theorem}
Then, given $f \in C(I_n)$ and $\epsilon > 0$, there is a sum $M(x)$ such that $|M(x) - f(x)| < \epsilon, \forall x \in I_n$.
\begin{proof}
Let $S \subset C(I_n)$ be the space of functions to which $M$ belongs. This means that $S$ is a linear subspace of $C(I_n)$. Assuming this were not true, according to the Hahn-Banach theorem and Riesz Representation theorem, there is a bounded linear functional on $C(I_n)$ that is zero on $S$. We write it as
\begin{align}
\int\limits_{I_n} \sigma(y^{\top} x + \theta) d \mu(x) = 0, ; \forall \y, \theta,
\end{align}
where $\mu$ is some Borel measure (required for $\sigma$ to be discriminatory). Since we assume $\sigma$ is discriminatory, therefore $\mu = 0$, our assumption is wrong, and the linear functional is zero everywhere (not just on $S$). This shows that sums of the form $M(x)$ are dense in $C(I_n)$ provided $\sigma$ is discriminatory and continuous.
\end{proof}
\citep{cybenko1989approximation} showed similar theorems and proofs for sigmoidal $\sigma$ (a function that is 1 on the positive real line, zero on the negative real line). However, they cautiously conclude that their proofs only show the \emph{existence} of such powerful networks but that finding the correct architecture for them is a question for feasibility.~\citep{shen2022optimal} showed the approximation rates for networks using the ReLU activation function in terms of the width and depth.~\citep{leshno1993multilayer} showed that as long as the chosen activation function is not a polynomial, multilayer perceptrons can approximate any function with arbitrary accuracy. While these results are from the perspective of increasing the width, the alternate perspective has also been explored. For example,~\citep{lu2017expressive, park2020minimum} derive the minimum width necessary for arbitrarily deep neural networks to be universal approximators; found to be $\text{max}(n + 1, m)$ where the functions being approximated are $f: \R^n \rightarrow \R^m$. The tradeoff between increasing the width and depth to fit different classes of functions is covered well by~\citep{telgarsky2016benefits}. In general, wide and shallow networks need exponentially more neurons in each layer to approximate a function with the same accuracy as a deep network~\cite[Theorem 1.15]{holstermann2023expressive}.

UATs have also been thoroughly investigated in the context of neural operators.~\citep{chen1995universal, chen1995approximation} came up with ``operator networks'' and showed that single layer neural networks can approximate with arbitrary accuracy any \emph{operator}. We explore UATs for two popular neural operator architectures, deep operator networks (DeepONet) and Fourier neural operator (FNO).~\citep{lu2019deeponet}, inspired from~\citep{chen1995universal}, came up with the DeepONet architecture (an inner product of two separate neural networks). The following is the UAT for DeepONets,
\paragraph{UAT for DeepONet}
\begin{theorem}
Given a nonlinear continuous operator $G$, an activation function $\sigma$ that belongs to the class of Tauber-Wiener functions (scalar valued continuous or discontinuous functions whose linear combinations are dense in the range of continuous functions $C([a, b])$), and $A$ and $U$ are the input and output function spaces respectively, then for any $\epsilon > 0$, there exists a positive integer $p$ such that
\begin{align}
\left| G(a)(y) - \sum\limits_{i=1}^{p} \boldsymbol{\beta}_i(a(X), \boldsymbol{\theta}_{\beta}) \sigma(\boldsymbol{\tau}_i(y, \boldsymbol{\theta}_{\tau})) \right| < \epsilon,
\end{align}
where $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ are the branch and trunk networks respectively parametrized by neural network parameters $\boldsymbol{\theta}_{\beta}$ and $\boldsymbol{\theta}_{\tau}$ respectively, $X$ is a set of points in $D_a$, and $y \in D_u$. $p$ is a hyperparameter and be seen as the number of basis functions of the DeepONet.
\end{theorem}

This UAT does not cover many areas of practical interest, where the input space may not be compact and assumptions on the continuity on $G$ may fail. It is also general enough that judiciously choosing $p$, cardinality of the set $X$ (number of location to sample functions in $A$ at), and the architecture of the branch and trunk networks is difficult. Recognizing this,~\citep{lanthaler2022error} comprehensively extended UATs for DeepONets and rigorously performed error analysis on them. Relaxing the norm to compute the error between $G$ and the DeepONet approximation, they were able to remove the compactness requirement that the UAT in~\citep{lu2019deeponet} and~\citep{chen1995universal} had and were able to apply the theorem to the approximation of nonlinear operators of the kind that appear in~\citep{lu2019deeponet}. This was a key step in explicitly showing how DeepONets can break the curse of dimensionality.

\citep{kovachki2021universal} presented a UAT for the Fourier neural operator (FNO) architecture. It is as follows:
\paragraph{UAT for FNO}
\begin{theorem}
FNOs can approximate an arbitrary operator $G$ with arbitrary accuracy; $\text{sup} \|G(a) - F(a)\|\limits_{H^d} < \epsilon, \forall a \in K$, where $F$ is the FNO, $G$ maps from functions in $H^s$ to functions in $H^d$, and $K \subset H^s$.
\begin{proof}
Let $G_N: H^s \rightarrow L^2, \; G_N(a) = P_N G(P_N a),$ where $P_N$ is the orthogonal Fourier projection operator. From this we have $\|G(a) - G_N(a)\|\limits_{L^2} \le \epsilon, \; \forall a \in K$. One needs to then show that $F$ can approximate $G_N$ with error $\epsilon$. Simplifying the next steps in~\citep{kovachki2021universal} proof, using Fourier dual operators we get the identity $G_N(a) = \F_N^{-1} \circ \hat{G}_N \circ \F_N (P_N a)$ where $\F$ and $\F^{-1}$ are the discrete Fourier and inverse Fourier transforms respectively. The key is to then show that FNOs can approximate each of the three operators, $\F_N^{-1}, \hat{G}_N,$ and $\F_N P_N$.
\end{proof}
\end{theorem}
{\bf Insights:} A common trend that emerges in UATs for both function and operator approximation by neural networks is to prove that the the architecture belongs to forms of functions that are dense in $L(\Omega)$ where $L$ is some function space to which the output space $U$ belongs and $\Omega \subset \R^u$. While the UAT results for function approximation directly help in choosing the architectural hyperparameters for neural networks (such as the width, height, and activation), it is not always trivial to do so for neural operators. For example, the UAT for FNOs does not include the lifting and projection layers (often parametrized with MLPs) explicitly and therefore provides no insight in picking their hyperparameters. Similarly, in the DeepONet UAT, while the theorem states that the output functions can be approximated with arbitrary accuracy, there are no bounds that describe the optimal hyperparameters for the branch and trunk networks \emph{simultaneously} (like the ones in~\citep{lu2017expressive, park2020minimum} for function approximation do).

Even though it is difficult to pick the precise architectural hyperparameters \emph{a priori} for DeepONets and FNOs, slightly relaxed analysis is still possible.~\citep{herrmann2024neural} proved that DeepONets with ReLU activation can approximate holomorphic operators with convergence rate $n^{1-s}$ (in the supremum norm) where $n$ is the number of trainable parameters and $s \sim 1/p$ where $p$ is the number of basis functions used in the DeepONet. Such analysis is more intuitive to perform on DeepONets than FNOs because DeepONets belong to a class of encoder-decoder style neural operator architectures~\citep{kovachki2024operator}, where encoding and decoding involve $A \approx \R^a$ and $\R^u \approx U$ respectively. For Lipschitz operators,~\citep{kovachki2024operator} also showed that if DeepONets and FNOs use standard MLPs, the number of parameters $n$ grows as $n \gtrsim \exp(c \epsilon^{-a/s})$, where $s$ is smoothness parameter.

Deriving universal approximation theorems for operators is an important but difficult task. Most current neural operator architectures do not lend themselves to UATs with which their optimal hyperparameters can be determined to achieve a desired error $\epsilon$. UATs for functions are useful in deriving only very general UATs for operators. In most encoder-decoder style architectures, the difficulty arises from the use of usually separate neural networks for encoding the input functions and the basis for output functions. A potential novel approach that may mitigate this is to use single network architectures (those similar to the ones described in~\citep{chen1995universal}). The challenge is to make something as expressive and accurate as the current state-of-the-art neural operators. The key to solving this may be an architecture that lies at the intersection of kernel based methods~\citep{batlle2024kernel} and neural networks, a personal research direction for the near future. Specifically for PDE applications, in such an architecture it would be possible to imbibe kernels with desirable conservation properties analytically (something current neural operators are known to struggle with~\citep{khorrami2024physics}), yet still benefit from the expressivity of neural networks.
%
\pagebreak
