{\bf Answer:}

{\bf Part a:} Let $\Omega \subset \R^{d}$ be a domain on which a vector-valued target function $\bof: \R^d \rightarrow \R^d$ is defined. Let $\phi: \R^d \times \R^d \rightarrow \R$ be a scalar valued kernel that is $C^2$-differentiable. We can then construct a \emph{matrix} valued divergence-free kernel $\Phidiv$ (whose columns are divergence free \emph{by construction}) as, $\Phidiv(\bx, \by) \coloneqq \curlx^{\top} \curly \; \phi(\bx, \by), \; \bx, \by \in \Omega$. Let $X = \{\bx_i \}\limits_{i=1}^{N}$ be a set of points in $\Omega$. Then, the global divergence-free interpolant $\bs$ is
\begin{align}
\bs(\bx) = \sum\limits_{i=1}^{N} \Phidiv(\bx, \bx_i) \; \bc_i, \label{divfreeinterp}
\end{align}
where $\bc_i \in \R^d$ are the interpolation coefficients. If instead we do partition-of-unity (PoU) interpolation,~\eqref{divfreeinterp} changes. We partition $\Omega$ into a set of $M$ overlapping patches $\{\Omega_j\}\limits_{j=1}^{M}$. The PoU approximant $\bspou$ is written as,
\begin{align}
\bspou(\bx) = \sum\limits_{k=1}^{M} \bs_k(\bx) \bc_k,
\end{align}
where $\bs_k(\bx) = w_k(\bx) \Phidiv(\bx, \bx_k)$, and $w_k$ are the compactly-supported blending functions. The problem here is that the basis functions $\bs_k$ are not divergence-free because of the multiplication with the weight functions. Hence, $\bspou$ is no longer divergence-free.

{\bf Part b:}~\citep{drake2021partition} used \emph{local} divergence-free and curl-free radial basis function (RBF) kernels to find the local scalar potential fields on each patch and blend them together with PoU to form a global scalar potential field. Then, the $\curlx^{\top} \curly$ operator is applied on the global scalar potential field to get the analytically divergence-free vector approximant. The key is to be able to extract a scalar potential $\psi$ out from~\eqref{divfreeinterp} in the following way (once the coefficients $\bc_i$ have been found),
\begin{align}
\bs(\bx) = \sum\limits_{i=1}^{N} \Phidiv(\bx, \bx_i) \bc_i = \sum\limits_{i=1}^{N} \left(\curlx^{\top} \curly \phi(\bx, \bx_i)\right) \bc_i = \underbrace{Q_{\bx} \nabla}_{\bL} \underbrace{\left(\sum\limits_{i=1}^{N} \nabla^{\top} \phi(\bx, \bx_i) Q_{\bx_i} \bc_i \right)}_{\psi(\bx)} = \bL(\psi(\bx)), \label{divfreeinterp_210}
\end{align}
where applying $Q_{\bx}$ to a vector in $\R^d$ gives the cross product of the unit normal $\mathbf{n}$ (in $d$ dimensions) with that vector, and $\psi$ is a scalar potential function that is unique up to an additive constant. $\bL = Q_{\bx} \nabla$ is the $\curlx$ operator. The idea behind the approach in this paper is to find and use the potential function $\psi_k$ from a patch's local divergence-free interpolant ${\bs_k^{_{\text{div}}}(\bx) = \sum\limits_{\forall \bx_t \in \Omega_k} \Phidiv(\bx, \bx_t) \bc_t}$. One approach is to say ${\tilde{\psi}(\bx) = \sum\limits_{k=1}^{M} w_k(\bx) \psi_k(\bx)}$ is the blended global PoU potential and then $\bL(\tilde{\psi})$ gives a global divergence-free approximant. The problem is that since the scalar potentials are unique only up to a constant, in the overlap regions the individual patch's scalar potential fields are not going to agree (they will be off up to the additive constant). Note that this problem does not occur in the normal PoU approximation because every patch's basis function is \emph{the same function} but only shifted (this is not true with the different $\psi_k$ calculated from~\eqref{divfreeinterp_210}). To rectify this, the authors shift each patch's $\psi_k$ by a constant $b_k$ such that $\psi_k + b_k \approx \psi_l + b_l$ for every patch $\Omega_l$ that overlaps with $\Omega_k$ (these constants $\{b_1, b_2, \dots, b_M\}$ need to be computed). Let $\tilde{\psi}_k$ denote a scaled and corrected \emph{local} potential function. Then, ${\tilde{\psi}(\bx) = \sum\limits_{k=1}^{M} w_k(\bx) \psi_k(\bx)}$ is the \emph{global} blended potential function. $\bL$ is then applied as follows to get a global PoU divergence-free approximant $\tilde{\mathbf{s}}_{_{\text{POU}}}$,
\begin{align}
\tilde{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}(\bx) \coloneqq \sum\limits_{k=1}^{M} \bL \left(w_k(\bx) \tilde{\psi}_k(\bx)\right) = \sum\limits_{k=1}^{M} w_k(\bx) \bs_k^{_{\text{div}}}(\bx) + \sum\limits_{k=1}^{M} \tilde{\psi}_k(\bx) \bL(w_k(\bx)). \label{divfreelocalpou}
\end{align}
%
The primary mathematical difficulty with extending this approach to 3D lies in~\eqref{divfreeinterp_210}. In 3D, the potential function associated with a divergence-free vector field is a vector field (this is due to the definition of the curl operator in 2D vs in 3D) that is unique up to an additive gradient of a harmonic scalar function. This disallows the key trick of this paper where only the difference between two potential functions' additive constants were accounted for in the overlapping regions.

{\bf Part c:} \citep{drake2021partition} solved the issue of $\bspou$ not being divergence-free. The main remaining difficulty is extending the approach to 3D problems. We derive a potential way to use machine learning to address this. The key idea is to approximate the vector-valued potential functions on each patch using a neural network. Let $\nn: \R^{3} \rightarrow \R^{3}$ denote a standard multilayer perceptron (MLP) neural network using some nonlinear activation function. We assume we already have the local divergence-free interpolants; $\bs_k^{_{\text{div}}}$ on the $k^{\text{th}}$ patch. Additionally, we use a different neural network on each patch denoted with $\nn_k$. We rewrite~\eqref{divfreelocalpou} as follows in this approach,
\begin{align}
\hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}(\bx) \coloneqq \sum\limits_{k=1}^{M} w_k(\bx) \bs_k^{_{\text{div}}}(\bx) + \sum\limits_{k=1}^{M} \nn_k(\bx) \bL(w_k(\bx)). \label{divfreelocalpou_nn}
\end{align}
Notice that we do not shift the networks $\nn_k$ yet for them to agree in the overlapping regions. It is not trivial nor cheap to find the gradient of the harmonic scalar function associated with $\nn_k$. Instead, we enforce that $\nn_k$ and $\nn_j$ for two patches $\Omega_k \cap \Omega_j \neq \emptyset$ (for an overlapping region of two patches) be equal to each other through a soft constraint. Let $\I_i$ denote the indices of the patches the point $\bx_i$ belongs to. Let $e_1$ be this loss function as follows,
\begin{align}
e_1 = \sum\limits_{i=1}^{N} \left\| \frac{1}{|\I_i|} \sum\limits_{k \in \I_i} \nn_k(\bx_i) \right\|_2.
\end{align}
For a given point, the expression inside the first sum goes to its minimum when all the neural networks have the same value, i.e. when all potential functions agree in the overlapping regions. Of course, this is not sufficient to train the networks since nothing is informing them about $\bof$. We use a second (supervised) loss function,
\begin{align}
e_2 = \sum\limits_{i=1}^{N} \left\| \hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}(\bx_i) - \bof(\bx_i) \right\|_2.
\end{align}
The total loss $\mathbf{e} = e_1 + e_2$ is then minimized using an off-the-shelf gradient based optimizer (eg: Adam, SGD, LBFGS). $\hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}$ can therefore overcome the limitation of the method in~\citep{drake2021partition} by way of machine learning. Additionally, since it is an extension of the~\citep{drake2021partition} method, it does not suffer from the problem in {\bf Part a}. $\hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}$ is a divergence-free approximant arising from the PoU blended vector potential function in $\R^3$.

{\bf Computational efficiency:} We can take certain implementation steps to compute the loss function efficiently at each training iteration. The weight functions $w_k$ are usually computed prior to training since they are not trainable. $\{\I_1, \I_2, \dots, \I_N\}$ can similarly be computed beforehand, with which we evaluate \emph{only} the $\nn_k$ and $\bs_{k}^{_{\text{div}}}$ for $k \in \I_i$ for every $\bx_i$. Finally, we can avoid computing $e_1$ on points that belong to a single patch.
%
\pagebreak
