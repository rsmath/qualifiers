{\bf Answer:}

{\bf Part a:} Let $\Omega \subset \R^{d}$ be a domain on which a vector-valued target function $\bof: \R^d \rightarrow \R^v$ is defined. Let $\phi: \R^d \times \R^d \rightarrow \R$ be a scalar valued kernel that is $C^2$-differentiable. We can then construct a \emph{matrix} valued divergence-free kernel $\Phidiv$ (whose columns are divergence free \emph{by construction}) as, $\Phidiv(\bx, \by) \coloneqq \curlx^{\top} \curly \; \phi(\bx, \by), \; \bx, \by \in \Omega$. Let $X = \{\bx_i \}\limits_{i=1}^{N}$ be a set of points in $\Omega$. Then, the global divergence-free interpolant $\bs$ is
\begin{align}
\bs(\bx) = \sum\limits_{i=1}^{N} \Phidiv(\bx, \bx_i) \; \bc_i, \label{divfreeinterp}
\end{align}
where $\bc_i \in \R^d$ are the interpolation coefficients. If instead we do partition-of-unity (PoU) interpolation,~\eqref{divfreeinterp} changes. We partition $\Omega$ into a set of $M$ overlapping patches $\{\Omega_j\}\limits_{j=1}^{M}$. The PoU approximant $\bspou$ is written as
\begin{align}
\bspou(\bx) = \sum\limits_{k=1}^{M} \bs_k(\bx) \bc_k,
\end{align}
where $\bs_k(\bx) = w_k(\bx) \Phidiv(\bx, \bx_k)$, and $w_k$ are the compactly-supported blending functions. The problem here is that the PoU basis functions $\bs_k$ are not a divergence-free basis because of the multiplication with the weight functions. Hence, $\bspou$ is no longer divergence-free.

{\bf Part b:}~\citep{drake2021partition} use \emph{local} divergence-free and curl-free radial basis function (RBF) kernels to find the local scalar potential fields on each patch and blend them together with PoU to form a global scalar potential field. Then, $\curlx^{\top} \curly$ is applied on the global scalar potential field to get the analytically divergence-free vector approximant. The key is to be able to extract a scalar potential out from~\eqref{divfreeinterp} in the following way (once the coefficients $\bc_i$ have been found),
\begin{align}
\bs(\bx) = \sum\limits_{i=1}^{N} \Phidiv(\bx, \bx_i) \bc_i = \sum\limits_{i=1}^{N} \left(\curlx^{\top} \curly \phi(\bx, \bx_i)\right) \bc_i = \underbrace{Q_{\bx} \nabla}_{\bL} \underbrace{\left(\sum\limits_{i=1}^{N} \nabla^{\top} \phi(\bx, \bx_i) Q_{\bx_i} \bc_i \right)}_{\psi(\bx)} = \bL(\psi(\bx)), \label{divfreeinterp_210}
\end{align}
where applying $Q_{\bx}$ to a vector in $\R^d$ gives the cross product of the unit normal $\mathbf{n}$ (in $d$ dimensions) with that vector, and $\psi$ is a scalar potential field. Therefore, $\bL = Q_{\bx} \nabla$ is the $\curlx$ operator. Since $\bL$ is a first order differential operator (in 2D), $\psi$ is unique up to an additive constant. The idea behind the approach in this paper is to find and use the potential field $\psi_k$ of every patch's local divergence-free interpolant ${\bs_k^{_{\text{div}}}(\bx) = \sum\limits_{\forall \bx_t \in \Omega_k} \Phidiv(\bx, \bx_t) \bc_t}$. One approach is to say ${\psi(\bx) = \sum\limits_{k=1}^{M} w_k(\bx) \psi_k(\bx)}$, is the global $\psi$ PoU blend and then $\bL(\psi)$ gives a global divergence-free approximant. The problem is that since the scalar potentials are unique only up to a constant, in the overlap regions the individual patch's scalar potential fields are not going to agree (they will be off up to the additive constant). Note that this problem does not occur in the normal PoU approximation because every patch's basis function is \emph{the same function} but only shifted (this is not true with the different $\psi_k$ which is calculated from~\eqref{divfreeinterp_210}). To rectify this, the authors shift each patch's $\psi_k$ by a constant $b_k$ such that $\psi_k + b_k \approx \psi_l + b_l$ for every patch $\Omega_l$ that overlaps with patch $\Omega_k$ (these constants $\{b_1, b_2, \dots, b_M\}$ need to be computed). Let $\tilde{\psi}_k$ denote a scaled and corrected \emph{local} potential field. Then, ${\tilde{\psi}(\bx) = \sum\limits_{k=1}^{M} w_k(\bx) \tilde{\psi}_k(\bx)}$, is the \emph{global} blended potential field. $\bL$ is then applied as follows to get a global PoU divergence-free approximant $\tilde{\mathbf{s}}_{_{\text{POU}}}$,
\begin{align}
\tilde{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}(\bx) \coloneqq \sum\limits_{k=1}^{M} \bL \left(w_k(\bx) \tilde{\psi}_k(\bx)\right) = \sum\limits_{k=1}^{M} w_k(\bx) \bs_k^{_{\text{div}}}(\bx) + \sum\limits_{k=1}^{M} \tilde{\psi}_k(\bx) \bL(w_k(\bx)). \label{divfreelocalpou}
\end{align}
%
The primary mathematical difficult with extending this approach to 3D lies in~\eqref{divfreeinterp_210}. In 3D, the potential function associated with a divergence-free vector field is a vector field (this is due to the definition of the curl operator in 2D vs in 3D) that is unique up to an additive gradient of a harmonic scalar function. This disallows the key trick of this paper where only the difference between two potential functions' additive constants were accounted for in the overlapping regions.

{\bf Part c:} \citep{drake2021partition} solved the issue of $\bspou$ not being divergence-free. The main remaining difficulty is extending the approach to 3D problems. We derive a potential way to use machine learning to address this. The key idea is to approximate the vector-valued potential fields on each patch using a network. Let $\nn: \R^{3} \rightarrow \R^{3}$ denote a standard multilayer perceptron (MLP) neural network using some nonlinear activation function. We assume we already have the local divergence-free interpolants; $\bs_k^{_{\text{div}}}$ on the $k^{\text{th}}$ patch. Additionally, we use a different neural network on each patch, denoted with $\nn_k$. We rewrite~\eqref{divfreelocalpou} as follows using the neural network approach,
\begin{align}
\hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}(\bx) \coloneqq \sum\limits_{k=1}^{M} w_k(\bx) \bs_k^{_{\text{div}}}(\bx) + \sum\limits_{k=1}^{M} \nn_k(\bx) \bL(w_k(\bx)). \label{divfreelocalpou_nn}
\end{align}
Notice that we do not shift $\nn_k$ yet for them to agree in the overlapping regions. It is not trivial nor cheap to find the gradient of the harmonic scalar function associated with $\nn_k$. Instead, we enforce that $\nn_k$ and $\nn_j$ for two patches $\Omega_k \cap \Omega_j \neq \emptyset$ be equal to each other through a soft constraint. Let $\I_i$ denote the indices of the patches point $\bx_i$ belongs to. Let $e_1$ be this loss function as follows,
\begin{align}
e_1 = \sum\limits_{i=1}^{N} \left\| \frac{1}{|\I_i|} \sum\limits_{k \in \I_i} \nn_k(\bx_i) \right\|_2.
\end{align}
For a given point, the expression inside the first sum goes to its minimum when all the neural networks have the same value, i.e. when all potential functions agree in the overlapping regions. Of course, this is not sufficient to train the networks since nothing is informing them about $\bof$. We use a second loss function,
\begin{align}
e_2 = \sum\limits_{i=1}^{N} \left\| \hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}(\bx_i) - \bof(\bx_i) \right\|_2.
\end{align}
The total loss, $\mathbf{e} = e_1 + e_2$ is minimized using a gradient based off-the-shelf optimizer (Adam, SGD, LBFGS). $\hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}$ therefore can overcome the limitation of the method in~\citep{drake2021partition} by way of machine learning. Additionally, since it is an extension of the~\citep{drake2021partition} method, it does not suffer from the problem in {\bf Part a}. $\hat{\mathbf{s}}^{_{\text{div}}}_{_{\text{POU}}}$ is a divergence-free basis function arising from the PoU blended vector potential in $\R^3$.

{\bf Computational efficiency:} We can take certain implementation steps to compute the loss function efficiently at each training iteration. The weight functions $w_k$ are usually computed prior to training since they do not change during training. $\{\I_1, \I_2, \dots, \I_N\}$ can similarly be computed beforehand, which then allows evaluating only $\nn_k$ and $\bs_{k}^{_{\text{div}}}$ for $k \in \I_i$ for a point $\bx_i$. Finally, we can avoid computing $e_1$ on points that belong to a single patch.
% put a neural network on each patch. compute derivatives either using RBF-FD (cite dt-pinn), or autograd, enforce derivatives to agree. 
% we can just use Drake et al's final equation that they use with our neural network approach, this way first problem does not need to be addressed individually. If doing Drake's method is difficult, we can address problem 1 instead, drive down divergence with a loss function of weight function times neural networks
% Or even better. have neural network spit out [w, phi] as an output, use curl [w, phi] analytically so its div-free, at the same time drive down sum w = 1 as either soft constraint (mention that we may need to tune hyperaprameter for this, not always easy, cite characterizing pinn failure mode paper), or
%     encode sum w = 1 has hard constraint? check out chalapathi2024scaling
% both difficulties are mutually exclusive.

%
\pagebreak
